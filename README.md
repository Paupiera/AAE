# Adversarial auotencoders for metagenomics binning 

1. Build model and train over the desired dataset: aae_fun_1.7.py 

2. Encode the dataset into the latent representation,
   cluster the latent representation and
   evaluate the clusters at strain, species and genus taxonomic levels: aae_latent_1.7.py



aae_fun_1.7.py**: 
    Options:   
        --h_n,hidden neurons, default=512
        --l_d,latent dimension, default = 32
        --s_l, scale discriminator loss  ,default = 0.005   
        --h_n_d, hidden neurons discriminator, if not defined h_n_d =h_n
        --batch_size, Inital batch size,( be aware that batch size will be doubled several times along training, more info at train_model function present in the same file)
        -e,epochs, default = 300
        -d, dataset, [Airways,Skin,Oral,Urog,Gi,Metahit]
        -G, grid search, (yes,not), important for the naming and log files, name files, etc
        -r, run or version, 1.0, 1.1, etc. Used to name files,logs and dirs
        --Test, if !='', run model over small data subset to error check

    Adversarial autoencoder, 
        Generator:
        input layer dim: (depths_i,tnfs_i)
        hidden layer dim:   h_n,h_n    LeakyReLU(BatchNorm1d(linear(LeakyReLU(BatchNorm1d(linear))))),    latent dim: latent_dim,2 Linear,         hidden layer dim: h_n,h_n    Linear(LeakyReLU(BatchNorm1d(linear(LeakyReLU(BatchNorm1d(linear))))))
        output: (softmax(depths_o),tnfs_o) 
        
        loss_f_G:  sse(tnf_i,tnf_o)*(alpha/(136*2)) + (1-alpha) / log(num_samples) * CE(depths_i,depths_o)   , alpha=0.15, 
        optimizer: Adam

        Discriminator:    
        hidden layer dim: h_n_d,h_n_d, Sigmoid(linear(LeakyReLU(linear(LeakyReLU(linear))))),  

        loss_f_D: BCE()

        optimizer: Adam

        Loss_G = s_l * loss_f_D(x~G,true) + (1-s_l) * loss_f_G
        
        Loss_D = 0.5 *(loss_f_D(x~N(0,I),true)+loss_f_D(x~G,false))
        

aae_latent_1.7.py**, 
    Main options: 
        -m, model name , 
        -d dataset, 
        -t (prec & rec threshold),
        -G (is model part of a grid search?), values=(yes,no)
        --load_model (Do we need to generate the latents?), values=(True,False)
        --clust, Clustering algorithm,values=(k23). keep in mind that to run Vamb-2 clustering algorithm, vamb-2 has to be installed and the same for vamb-3 clustering algorithm
        
    - Load data:
        depths,tnf
        contignames, OTUs, contig_lenghts
    - Load encoder
    - Get dataset latent representation using the get_latents function imported from get_latents.py
    - Cluster latent representation using aae_clustering_(2,3,kmeans).py     
    - Benchmark the clusters using aae_benchmark_2.py for all clusters  
    




aae_clustering_2.py,
    Options:
        -L latents_path
        -d dataset

    Cluster latent representations using vamb-2 clustering algorithm, save post processed and not postprocessed clusters

aae_clustering_3.py, 
    Same than aae_clustering_2.py, but in that case we use vamb-3 clustering algotrithm, 

aae_clustering_kmeans.py, 
    Same than aae_clustering_2.py, but in that case we use sklearn MiniBatchkMeans c clustering algotrithm, (python sklearn: “MiniBatchKMeans(n_clusters=750, random_state=0, batch_size=4096, max_iter=25, init_size=20000, reassignment_ratio=0.02)”.) 
    



aae_benchmark.py,
    Options:
        -C: Clusters path
        -d: dataset
    Output and plot bins above preset prec along recall values (0.5,0.6,...,0.99) for aae, vamband metabat, for each tax level

aae_benchmark_2.py**,
    Options:
        -C path_clusters
        -d dataset, 
        -p precision threshold
        -r recall threshold
        -G (is model part of a grid search?), values=(None,!=None)
        --Log_name Log_name_results  
    
    function arguments:
        - path_clusters
        - dataset
        - precision, default=0.9
        - recall, default=0.9

    Output bins number above the given thresholds at different taxonoimc ranks and if -G is provided, output results in logfile

       


aae_benchmark_4.py
    Given the model, generates a 3x3x3 array where 1-d (row) is precision (0.9,0.95,0.99), 2-d is recall (0.9,0.95,0.99) and 3d is tax rank (OTU,Species,Genus)


aae_distances_2.4.py: compare Vamb and AAE bins
    Options:
        -d: model
        --plot: Plot model bins comparison, type= bool
        --compare: compute models bin comparison and comparison and which kind: by contig id or by bps?, type=str, possible values:n,id,bps 
        --clust_alg: clustering algorithm used 
        --plot: Plot Zscores and distances for both models
    distances function:
        function workflow:
            1.load the clusters generated by the model(aae or vamb) as well as the reference data
            2. Obtain the clusters with prec and recall above the prec and recall thresholds
            
        Output: 
            model_bins object
            model_bins_above_thr: dictionary[bin_name]=bin_contigs , only containing bins above prec and rec thrs
            model_bins_above_thr_p_r: dictionary[bin_name] = (prec,recall) , only containing bins above prec and rec thrs

            recallof: recallof[binname][genome.name] = recall
            precisionof: precisionof[binname][genome.name] = precision
       
    Comparison: 
        - for model bins above threshold, output the num of bins that overlap given contig id content or bps overlap
        - for all model bins , output the num of bins that overlap given id content or bps overlap

    Plot: map aae and vamb bins given contig similarity and plot
        - Recall
        - Precision
        - F1

    Plot filename: distances_2.4_<model_name>_t_<threshold>_comparison_<comparison>.png
            
aae_distances_2.5.py: similar than aae_distances_2.4.py but in that case we can decide both models to compare, is not aae vs vamb always, it can be aae vs aae_2, it set it to compare the bins obtained from from the same latents appliyng different clustering algorithms.

aae_distances_2.2.py
    Options:
        -d: model
        -D: model directory
        --plot: Plot Z-scores and distances for both models
    distances function:
        
        function workflow:
            1.load the clusters generated by the model(aae or vamb) as well as the reference data
            2. given the precision and recall ranges, obtain the clusters with prec and recall above the thresholds for a given genome 
            3. Get the num of contigs per cluster
            4. Get the contigs length for the clusters above the p and r thresholds
            5. Get the bin length for the bins above the p and r thresholds
            6. Compute the distances between contigs inside the same cluster and outside
            7. display distribution of 1,2, 3 and 4 and save the plot 
        Output:
            _dictionaries_list: [_contigs_per_bin_num[P=0.9,R=0.9]=[contigs_bin_a_counts,contigsbin_b_counts,etc],
                                _contigs_lengths[P=0.9,R=0.9]=cont_lenght_1,cont_lenght_2,...,
                                _bin_lengths[P=0.9,R=0.9]=[bin_a_lenght_bps,bin_b_lenght_bps,...,]
                                _own_distances[P=0.9,R=0.9]=[distance_con_a_bin_a,distance_con_b_bin_a,..,],
                                _far_distances[P=0.9,R=0.9]=[distance_con_a_bin_a,distance_con_a_bin_c,..,],
                                _contigs_z_cluster[P=0.9,R=0.9]=[Z_score_contig_a_clust,...,] contig_a belongs to cluster with prec & rec > threshols,

                                _contigs_far[P=0.9,R=0.9]=[Z_score_contig_a_clust,...,] contig_a does not belong to cluster with prec & rec > threshols,


                                _contigs_standard_z[P=0.9,R=0.9]=[Z_score_a] where a is sampled from N(I,0),
                                _model_contigs[P=0.9,R=0.9]=[contig_a,contig_b,...,] contig_a belongs to clusters with p&r > thr
                                ]
            prec_rec_list=['P=0.9 R=0.9','P=0.9 R=0.95',...,'P=0.99 R=0.99'] 
            _total_contigs_num
        
            Distances plot filename (single model): distances_2.2<dataset_name>_<model_name>clust_alg.png
        
        
        Distances plot filename (both models): distances_2.2_<dataset_name>_clust_alg_both_.png
        
        Z-scores plot filename (both models): Probs_2.2_<dataset_name>_clust_alg_both_.png


data_loader.py: 
    """Create a DataLoader and a contig mask from RPKM and TNF.

    The dataloader is an object feeding minibatches of contigs to the model.
    The data are normalized versions of the input datasets, with zero-contigs,
    i.e. contigs where a row in either TNF or RPKM are all zeros, removed.
    The mask is a boolean mask designating which contigs have been kept.

    Inputs:
        rpkm: RPKM matrix (N_contigs x N_samples)
        tnf: TNF matrix (N_contigs x 136)
        batchsize: Starting size of minibatches for dataloader
        destroy: Mutate rpkm and tnf array in-place instead of making a copy. ????#Save memory by destroying matrix while clustering
        Cuda: Pagelock memory of dataloader (use when using GPU acceleration)

    Outputs:
        DataLoader: An object feeding data to the model
        mask: A boolean mask of which contigs are kept
    """






